param (
    [switch]$VerifyGpu,
    [switch]$RunTest,
    [switch]$BuildOnly,
    [string]$ModelPath
)

# PowerShell script to test GPU/TorchSharp integration
Write-Host "üöÄ GPU Local LLM Test Script" -ForegroundColor Cyan

# Default model path if not specified
if (-not $ModelPath) {
    $ModelPath = "./models/llama-2-7b-chat.Q4_K_M.gguf"
}

# Set environment variables for TorchSharp
$env:CUDA_VISIBLE_DEVICES = "0"
$env:PYTORCH_CUDA_ALLOC_CONF = "max_split_size_mb:128"

function Test-GpuAvailability {
    Write-Host "üîç Verifying GPU availability with TorchSharp..." -ForegroundColor Yellow
    
    # Build and run the GPU detection tool
    try {
        $outputPath = "./bin/GpuDetection"
        
        # Ensure directory exists
        if (-not (Test-Path $outputPath)) {
            New-Item -Path $outputPath -ItemType Directory -Force | Out-Null
        }
        
        # Create temporary C# file for GPU detection
        $gpuDetectionCode = @'
using System;
using TorchSharp;
using static TorchSharp.torch;

namespace GpuDetection
{
    class Program
    {
        static void Main(string[] args)
        {
            Console.WriteLine("üîç GPU Detection with TorchSharp");
            
            try
            {
                bool cudaAvailable = cuda.is_available();
                Console.WriteLine($"‚úÖ CUDA available: {cudaAvailable}");
                
                if (cudaAvailable)
                {
                    int deviceCount = cuda.device_count();
                    Console.WriteLine($"üìä CUDA device count: {deviceCount}");
                    
                    for (int i = 0; i < deviceCount; i++)
                    {
                        cuda.set_device(i);
                        Console.WriteLine($"  - Device {i}: {cuda.get_device_name(i)}");
                    }
                    
                    // Create a simple tensor on GPU
                    using (var tensor = torch.ones(new long[] { 3, 3 }, device: cuda.current_device()))
                    {
                        Console.WriteLine($"üìà Test tensor on GPU created successfully");
                        Console.WriteLine($"   Shape: {string.Join("x", tensor.shape)}");
                        Console.WriteLine($"   Device: {tensor.device_type}");
                    }
                }
                else
                {
                    Console.WriteLine("‚ö†Ô∏è CUDA is not available on this system");
                }
            }
            catch (Exception ex)
            {
                Console.WriteLine($"‚ùå Error: {ex.Message}");
                Console.WriteLine($"Stack trace: {ex.StackTrace}");
            }
        }
    }
}
'@
        
        $gpuDetectionPath = Join-Path $outputPath "GpuDetection.cs"
        Set-Content -Path $gpuDetectionPath -Value $gpuDetectionCode
        
        # Create project file
        $projectCode = @'
<Project Sdk="Microsoft.NET.Sdk">

  <PropertyGroup>
    <OutputType>Exe</OutputType>
    <TargetFramework>net8.0</TargetFramework>
  </PropertyGroup>

  <ItemGroup>
    <PackageReference Include="TorchSharp" Version="0.102.5" />
  </ItemGroup>

</Project>
'@
        
        $projectPath = Join-Path $outputPath "GpuDetection.csproj"
        Set-Content -Path $projectPath -Value $projectCode
        
        # Build and run
        Write-Host "üî® Building GPU detection tool..." -ForegroundColor Yellow
        dotnet build $projectPath -c Release
        
        if ($LASTEXITCODE -ne 0) {
            Write-Host "‚ùå Build failed with exit code $LASTEXITCODE" -ForegroundColor Red
            return $false
        }
        
        Write-Host "üèÉ Running GPU detection..." -ForegroundColor Yellow
        dotnet run --project $projectPath --configuration Release
        
        if ($LASTEXITCODE -ne 0) {
            Write-Host "‚ùå GPU detection failed with exit code $LASTEXITCODE" -ForegroundColor Red
            return $false
        }
        
        return $true
    }
    catch {
        Write-Host "‚ùå Error verifying GPU: $_" -ForegroundColor Red
        return $false
    }
}

function New-GpuLocalLlmService {
    Write-Host "üî® Building GpuLocalLLMService..." -ForegroundColor Yellow
    
    try {
        # Build the LocalLLM project
        dotnet build ./src/CxLanguage.LocalLLM/CxLanguage.LocalLLM.csproj -c Release
        
        if ($LASTEXITCODE -ne 0) {
            Write-Host "‚ùå Build failed with exit code $LASTEXITCODE" -ForegroundColor Red
            return $false
        }
        
        Write-Host "‚úÖ Build successful" -ForegroundColor Green
        return $true
    }
    catch {
        Write-Host "‚ùå Error building GpuLocalLLMService: $_" -ForegroundColor Red
        return $false
    }
}

function Start-LlmTest {
    param (
        [string]$ModelPath
    )
    
    Write-Host "üèÉ Running LLM test with model: $ModelPath" -ForegroundColor Yellow
    
    try {
        # Create temporary test project
        $outputPath = "./bin/LlmTest"
        
        # Ensure directory exists
        if (-not (Test-Path $outputPath)) {
            New-Item -Path $outputPath -ItemType Directory -Force | Out-Null
        }
        
        # Create test program
        $testCode = @"
using System;
using System.Threading.Tasks;
using Microsoft.Extensions.DependencyInjection;
using Microsoft.Extensions.Logging;
using CxLanguage.LocalLLM;

namespace LlmTest
{
    class Program
    {
        static async Task Main(string[] args)
        {
            Console.WriteLine("üöÄ GPU Local LLM Test");
            
            try
            {
                // Set up dependency injection
                var services = new ServiceCollection();
                
                // Add logging
                services.AddLogging(builder => 
                {
                    builder.SetMinimumLevel(LogLevel.Information);
                    builder.AddConsole();
                });
                
                // Add GPU local LLM service
                services.AddGpuLocalLlm();
                
                // Build service provider
                var serviceProvider = services.BuildServiceProvider();
                
                // Get local LLM service
                var llmService = serviceProvider.GetRequiredService<ILocalLLMService>();
                
                // Initialize
                Console.WriteLine("üöÄ Initializing service...");
                bool initialized = await llmService.InitializeAsync();
                Console.WriteLine($"‚úÖ Initialization: {initialized}");
                
                // Load model
                string modelPath = "$ModelPath";
                Console.WriteLine($"üì• Loading model: {modelPath}");
                bool modelLoaded = await llmService.LoadModelAsync(modelPath);
                Console.WriteLine($"‚úÖ Model loaded: {modelLoaded}");
                
                if (modelLoaded)
                {
                    var modelInfo = llmService.ModelInfo;
                    Console.WriteLine($"üìä Model info: {modelInfo?.Name}, {modelInfo?.Architecture}, {modelInfo?.SizeBytes / (1024 * 1024)}MB");
                }
                
                // Generate
                string prompt = "Explain how consciousness works in 3 sentences.";
                Console.WriteLine($"üß† Generating with prompt: {prompt}");
                
                string result = await llmService.GenerateAsync(prompt);
                Console.WriteLine($"üìù Result: {result}");
                
                // Stream
                Console.WriteLine("\nüì∫ Streaming with prompt: Write a short poem about artificial consciousness.");
                
                await foreach (var token in llmService.StreamAsync("Write a short poem about artificial consciousness."))
                {
                    Console.Write(token);
                }
                
                Console.WriteLine("\n\n‚úÖ Streaming complete");
                
                // Unload
                Console.WriteLine("üì§ Unloading model...");
                await llmService.UnloadModelAsync();
                Console.WriteLine("‚úÖ Model unloaded");
                
                // Dispose
                if (llmService is IDisposable disposable)
                {
                    Console.WriteLine("üßπ Disposing service...");
                    disposable.Dispose();
                    Console.WriteLine("‚úÖ Service disposed");
                }
            }
            catch (Exception ex)
            {
                Console.WriteLine($"‚ùå Error: {ex.Message}");
                Console.WriteLine($"Stack trace: {ex.StackTrace}");
            }
        }
    }
}
"@
        
        $testPath = Join-Path $outputPath "Program.cs"
        Set-Content -Path $testPath -Value $testCode
        
        # Create project file
        $projectCode = @'
<Project Sdk="Microsoft.NET.Sdk">

  <PropertyGroup>
    <OutputType>Exe</OutputType>
    <TargetFramework>net8.0</TargetFramework>
  </PropertyGroup>

  <ItemGroup>
    <PackageReference Include="Microsoft.Extensions.DependencyInjection" Version="8.0.0" />
    <PackageReference Include="Microsoft.Extensions.Logging" Version="8.0.0" />
    <PackageReference Include="Microsoft.Extensions.Logging.Console" Version="8.0.0" />
  </ItemGroup>

  <ItemGroup>
    <ProjectReference Include="..\..\src\CxLanguage.LocalLLM\CxLanguage.LocalLLM.csproj" />
  </ItemGroup>

</Project>
'@
        
        $projectPath = Join-Path $outputPath "LlmTest.csproj"
        Set-Content -Path $projectPath -Value $projectCode
        
        # Build and run
        Write-Host "üî® Building LLM test..." -ForegroundColor Yellow
        dotnet build $projectPath -c Release
        
        if ($LASTEXITCODE -ne 0) {
            Write-Host "‚ùå Build failed with exit code $LASTEXITCODE" -ForegroundColor Red
            return $false
        }
        
        Write-Host "üèÉ Running LLM test..." -ForegroundColor Yellow
        dotnet run --project $projectPath --configuration Release
        
        if ($LASTEXITCODE -ne 0) {
            Write-Host "‚ùå LLM test failed with exit code $LASTEXITCODE" -ForegroundColor Red
            return $false
        }
        
        return $true
    }
    catch {
        Write-Host "‚ùå Error running LLM test: $_" -ForegroundColor Red
        return $false
    }
}

# Main execution flow
if ($VerifyGpu) {
    Test-GpuAvailability
}
elseif ($BuildOnly) {
    New-GpuLocalLlmService
}
elseif ($RunTest) {
    if (New-GpuLocalLlmService) {
        Start-LlmTest -ModelPath $ModelPath
    }
}
else {
    # Default: Verify GPU, build, and run test
    if (Test-GpuAvailability) {
        if (New-GpuLocalLlmService) {
            Start-LlmTest -ModelPath $ModelPath
        }
    }
}

Write-Host "‚úÖ GPU Local LLM Test Script Complete" -ForegroundColor Cyan
