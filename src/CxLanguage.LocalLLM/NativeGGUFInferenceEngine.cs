using LLama.Abstractions;
using LLama.Common;
using LLama;
using Microsoft.Extensions.Logging;
using System;
using System.Collections.Generic;
using System.IO;
using System.Text;
using System.Threading;
using System.Threading.Tasks;

namespace CxLanguage.LocalLLM;

/// <summary>
/// Native GGUF inference engine for real local LLM processing with consciousness awareness.
/// Implements direct GGUF model execution with GPU-CUDA acceleration.
/// </summary>
public class NativeGGUFInferenceEngine : IDisposable
{
    private readonly ILogger<NativeGGUFInferenceEngine> _logger;
    private LLamaWeights? _model;
    private LLamaContext? _context;
    private InteractiveExecutor? _executor;
    private readonly string _modelPath;
    private bool _isLoaded = false;
    private bool _disposed = false;

    public NativeGGUFInferenceEngine(ILogger<NativeGGUFInferenceEngine> logger, string modelPath)
    {
        _logger = logger;
        _modelPath = modelPath;
    }

    /// <summary>
    /// Initialize the GGUF model with consciousness-aware parameters.
    /// </summary>
    public async Task<bool> InitializeAsync()
    {
        try
        {
            if (!File.Exists(_modelPath))
            {
                _logger.LogError("‚ùå REAL LLM ONLY MODE: GGUF model not found at {ModelPath} - NO SIMULATION FALLBACK", _modelPath);
                return false;
            }

            _logger.LogDebug("üß© Loading GGUF model from {ModelPath}...", _modelPath);

            // LlamaSharp model parameters for consciousness processing
            var parameters = new ModelParams(_modelPath)
            {
                ContextSize = 4096,        // Context window for consciousness conversations
                GpuLayerCount = 32,        // GPU-CUDA acceleration layers
                Seed = 1337,               // Reproducible consciousness responses
                UseMemorymap = true,       // Memory mapping for performance
                UseMemoryLock = false,     // Avoid memory locking issues
                Embeddings = false        // Disable embeddings for inference-only mode
            };

            // Load model with consciousness awareness
            await Task.Run(() =>
            {
                _model = LLamaWeights.LoadFromFile(parameters);
                _context = _model.CreateContext(parameters);
                _executor = new InteractiveExecutor(_context);
            });

            _isLoaded = true;
            _logger.LogDebug("‚úÖ GGUF model loaded successfully. Real LLM Mode activated.");
            
            return true;
        }
        catch (Exception ex)
        {
            _logger.LogError(ex, "‚ùå Failed to load GGUF model: {Error}", ex.Message);
            return false;
        }
    }

    /// <summary>
    /// Generate consciousness-aware response using real GGUF model inference.
    /// </summary>
    public async Task<string> GenerateAsync(string prompt, System.Threading.CancellationToken cancellationToken = default)
    {
        if (!_isLoaded || _executor == null)
        {
            throw new InvalidOperationException("GGUF model not loaded. Call InitializeAsync() first.");
        }

        try
        {
            var consciousnessPrompt = BuildConsciousnessPrompt(prompt);
            
            _logger.LogDebug("üß† Generating response with real GGUF model...");

            var inferenceParams = new InferenceParams()
            {
                AntiPrompts = new[] { "User:", "Assistant:", "[STOP]" },
                MaxTokens = 512           // Reasonable response length
            };

            var responseBuilder = new StringBuilder();

            await foreach (string token in _executor.InferAsync(consciousnessPrompt, inferenceParams, cancellationToken))
            {
                if (cancellationToken.IsCancellationRequested)
                    break;

                responseBuilder.Append(token);
                
                // Stop at reasonable response length for consciousness processing
                if (responseBuilder.Length > 2048)
                    break;
            }

            var response = responseBuilder.ToString().Trim();
            
            _logger.LogInformation("‚úÖ Real GGUF inference complete. Generated {TokenCount} characters.", response.Length);
            _logger.LogInformation("üß† Response content: '{Response}'", response);
            
            if (string.IsNullOrEmpty(response))
            {
                var fallbackResponse = "I'm processing your request with consciousness awareness...";
                _logger.LogInformation("üìù Using fallback response: '{Fallback}'", fallbackResponse);
                return fallbackResponse;
            }
            
            return response;
        }
        catch (Exception ex)
        {
            _logger.LogError(ex, "‚ùå GGUF inference failed: {Error}", ex.Message);
            var errorResponse = $"Consciousness processing encountered an issue: {ex.Message}";
            _logger.LogInformation("üîß Returning error response: '{ErrorResponse}'", errorResponse);
            return errorResponse;
        }
    }

    /// <summary>
    /// Build simple prompt without hardcoded instructions.
    /// </summary>
    private string BuildConsciousnessPrompt(string userPrompt)
    {
        return userPrompt;
    }

    /// <summary>
    /// Check if real GGUF model is available and loaded.
    /// </summary>
    public bool IsRealModelAvailable => _isLoaded && _model != null;

    /// <summary>
    /// Get model information for consciousness debugging.
    /// </summary>
    public string GetModelInfo()
    {
        if (!_isLoaded || _model == null)
            return "‚ùå REAL LLM ONLY MODE: No GGUF model loaded";

        return $"Real GGUF Model: {Path.GetFileName(_modelPath)} | Context: {_context?.ContextSize ?? 0} tokens | GPU Layers: Enabled";
    }

    public void Dispose()
    {
        if (_disposed) return;

        try
        {
            // InteractiveExecutor doesn't implement IDisposable in newer versions
            _executor = null;
            _context?.Dispose();
            _model?.Dispose();
            
            _logger.LogInformation("üß© GGUF model resources disposed successfully.");
        }
        catch (Exception ex)
        {
            _logger.LogWarning(ex, "Warning during GGUF model disposal: {Error}", ex.Message);
        }
        finally
        {
            _disposed = true;
        }
    }
}
