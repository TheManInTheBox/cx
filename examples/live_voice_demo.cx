// 🎯 LIVE VOICE DEMO: Real microphone → AI agent → Real speakers
// GOAL: Actual voice conversation with microphone input and speaker output

conscious LiveVoiceAgent
{
    realize(self: conscious)
    {
        print("🎤 LiveVoiceAgent ready for REAL voice conversation");
        learn self;
        emit agent.ready { name: self.name, capability: "live_voice" };
    }
    
    // Handle real voice input from microphone
    on voice.input.received (event)
    {
        print("👂 Agent heard via microphone: " + event.text);
        print("🧠 Processing your voice input...");
        
        // Generate contextual response based on what user said
        var contextualResponse = "I heard you say: " + event.text + ". That's a great question! I'm here to help you with whatever you need.";
        
        print("💭 AI Response: " + contextualResponse);
        print("🗣️ Speaking response to your speakers...");
        
        // Send to Azure for voice synthesis
        emit realtime.text.send { 
            text: contextualResponse,
            deployment: "gpt-4o-mini-realtime-preview"
        };
    }
    
    // Handle voice synthesis and play through speakers
    on realtime.audio.response (event)
    {
        print("🔊 Audio response received from Azure");
        print("🔊 Playing through your default speakers...");
        
        // Play audio with explicit device settings
        emit voice.output.play { 
            audioData: event.audioData,
            format: "pcm16",
            sampleRate: 24000,
            channels: 1,
            volume: 1.0,
            deviceId: "default"
        };
        
        // Check if synthesis is complete
        is {
            context: "Is the voice response fully synthesized and playing?",
            evaluate: "Audio completion status check",
            data: { isComplete: event.isComplete },
            handlers: [ voice.playback.complete ]
        };
    }
    
    // Handle playback completion
    on voice.playback.complete (event)
    {
        print("✅ Voice playback complete - listening for next input");
        print("🎤 Say something else to continue the conversation...");
        
        // Keep listening for more input
        emit voice.input.start { 
            device: "default_microphone",
            timeout: 30000  // 30 seconds
        };
    }
}

// System startup
on system.start (event)
{
    print("🚀 Starting LIVE Voice Conversation Demo");
    print("🎯 Real microphone input → AI agent → Real speaker output");
    print("📋 Make sure your microphone and speakers are working!");
    
    // Connect to Azure Realtime API
    emit realtime.connect { demo: "live_voice_conversation" };
}

// Handle Azure connection
on realtime.connected (event)
{
    print("✅ Connected to Azure Realtime API");
    
    // Create voice session
    emit realtime.session.create { 
        deployment: "gpt-4o-mini-realtime-preview",
        mode: "voice"
    };
}

// Start live voice input when session is ready
on realtime.session.created (event)
{
    print("✅ Azure voice session ready");
    print("🎤 Starting microphone listening...");
    print("📢 SPEAK NOW: Say something to test the voice pipeline!");
    
    // Start real microphone input
    emit voice.input.start { 
        device: "default_microphone",
        timeout: 15000,  // 15 seconds to speak
        sensitivity: 0.5
    };
}

// Handle microphone timeout
on voice.input.timeout (event)
{
    print("⏰ Microphone timeout - no speech detected");
    print("🔄 Restarting voice input - try speaking again...");
    
    emit voice.input.start { 
        device: "default_microphone",
        timeout: 15000
    };
}

// Graceful shutdown after successful demo
on voice.conversation.complete (event)
{
    print("🎉 Live voice conversation demo completed successfully!");
    emit system.shutdown { reason: "Live voice demo completed" };
}

on system.shutdown (event)
{
    print("🔌 Live Voice Demo complete: " + event.reason);
}

// Create the live voice agent
var liveAgent = new LiveVoiceAgent({ name: "LiveVoiceAgent" });
