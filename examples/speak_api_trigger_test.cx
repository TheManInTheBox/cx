// Testing speak API with actual Azure Realtime API integration
// This will trigger the real voice synthesis through Azure

class VoiceAPIAgent
{
    name: string = "VoiceAPI";
    
    function triggerSpeakAPI()
    {
        print("ğŸ”Š Triggering Azure Realtime Speak API...");
        
        // First establish Azure Realtime connection
        emit realtime.connect { demo: "speak_api_test" };
    }
    
    function testCognitiveSpeakCall()
    {
        print("ğŸ§  Testing cognitive speak function...");
        
        // This will emit ai.speak.request event
        speak { text: "Hello, this is a test of the Azure Realtime speak API" };
    }
    
    // Handle Azure Realtime API connection
    on realtime.connected (event)
    {
        print("âœ… Connected to Azure Realtime API");
        
        // Create voice session for speech synthesis
        emit realtime.session.create { 
            deployment: "gpt-4o-mini-realtime-preview",
            mode: "voice"
        };
    }
    
    // Handle session creation
    on realtime.session.created (event)
    {
        print("âœ… Azure voice session created");
        
        // Now test both cognitive speak call and direct API call
        this.testCognitiveSpeakCall();
        
        print("ğŸ”Š Also testing direct Azure Realtime text send...");
        // Direct Azure Realtime API call
        emit realtime.text.send { 
            text: "This is a direct call to Azure Realtime API",
            deployment: "gpt-4o-mini-realtime-preview"
        };
    }
    
    // Handle cognitive speak requests (bridge to Azure API)
    on ai.speak.request (event)
    {
        print("ğŸŒ‰ Bridging cognitive speak to Azure Realtime API");
        print("Text to speak: " + event.text);
        
        // Bridge cognitive speak to Azure Realtime API
        emit realtime.text.send {
            text: event.text,
            deployment: "gpt-4o-mini-realtime-preview"
        };
    }
    
    // Handle Azure text responses
    on realtime.text.response (event)
    {
        print("ğŸ“ Azure text response: " + event.content);
        print("Response complete: " + event.isComplete);
        
        if (event.isComplete)
        {
            print("âœ… Text response generation complete");
        }
    }
    
    // âœ… FIXED: Handle Azure audio responses with proper type safety
    on realtime.audio.response (event)
    {
        if (event.audioData != null)
        {
            print("ğŸµ VOICE SYNTHESIS COMPLETE! Audio data received:");
            print("Audio data available: true");
            print("Audio data type: " + typeof(event.audioData));
        }
        else
        {
            print("ğŸµ Audio response received (no data)");
        }
        
        print("Audio complete: " + event.isComplete);
        
        if (event.isComplete)
        {
            print("âœ… Voice synthesis fully complete!");
        }
    }
    
    // Handle any connection issues
    on realtime.error (event)
    {
        print("âŒ Azure Realtime API error: " + event.message);
    }
}

// Create and test the voice API agent
var voiceAgent = new VoiceAPIAgent();

print("=== AZURE REALTIME SPEAK API TEST ===");
print("This will trigger actual voice synthesis through Azure OpenAI");
print("");

// Start the API test
voiceAgent.triggerSpeakAPI();

print("");
print("ğŸ”Š Speak API test initiated - waiting for Azure responses...");
