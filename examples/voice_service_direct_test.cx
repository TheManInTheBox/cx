// Voice Output Service Test - Priority Queuing & Direct Hardware Control
// Focus: Test voice synthesis and hardware audio playback with debugging

conscious VoiceTestAgent
{
    realize(self: conscious)
    {
        learn self;
        emit agent.ready { name: self.name };
    }
    
    on test.start (event)
    {
        print("ğŸ”Š Starting voice output service test");
        print("ï¿½ Focus: Voice synthesis â†’ Hardware playback");
        
        // Test 1: Direct voice synthesis
        emit realtime.connect { demo: "voice_output_test" };
    }
    
    on realtime.connected (event)
    {
        print("âœ… Connected to Azure Realtime API");
        emit realtime.session.create { mode: "voice" };
    }
    
    on realtime.session.created (event)
    {
        print("âœ… Voice session created");
        print("ğŸ—£ï¸ Requesting voice synthesis...");
        
        // Request voice synthesis with clear debugging
        emit realtime.text.send { 
            text: "Testing voice output. You should hear this spoken audio."
        };
    }
    
    on realtime.voice.response (event)
    {
        is {
            context: "Should process voice data?",
            evaluate: "Voice data is available for playback",
            data: { audioData: event.audioData },
            handlers: [ voice.data.received ]
        };
        
        is {
            context: "Should log completion status?",
            evaluate: "Voice synthesis is complete",
            data: { isComplete: event.isComplete },
            handlers: [ voice.synthesis.completed ]
        };
    }
    
    on voice.data.received (event)
    {
        print("ğŸ”Š Received voice data - routing to VoiceOutputService");
        print("ğŸ“Š Voice data type: " + typeof(event.audioData));
        
        // Route voice data to VoiceOutputService for playback
        emit voice.output.play { 
            audioData: event.audioData,
            sampleRate: 24000,
            channels: 1
        };
    }
    
    on voice.synthesis.completed (event)
    {
        print("ğŸµ Voice synthesis complete!");
    }
    
    on voice.output.queued (event)
    {
        print("ğŸ”„ Voice request queued:");
        print("  Request ID: " + event.requestId);
        print("  Priority: " + event.priority);
        print("  Audio Length: " + event.audioLength + " bytes");
    }
    
    on voice.output.started (event)
    {
        print("âœ… Voice output started successfully:");
        print("  Request ID: " + event.requestId);
        print("  Priority: " + event.priority);
        print("  Audio length: " + event.audioLength + " bytes");
        print("  Sample rate: " + event.sampleRate + "Hz");
        print("  Channels: " + event.channels);
    }
    
    on voice.output.completed (event)
    {
        print("âœ… Voice output completed successfully:");
        print("  Request ID: " + event.requestId);
        print("  Priority: " + event.priority);
        print("  Timestamp: " + event.timestamp);
        
        is {
            context: "Should display exception information?",
            evaluate: "Event contains exception data",
            data: { exception: event.exception },
            handlers: [ exception.display ]
        };
    }
    
    on exception.display (event)
    {
        print("âš ï¸ Exception during playback: " + event.exception);
    }
    
    on voice.output.error (event)
    {
        print("âŒ Voice output error: " + event.error);
        print("Timestamp: " + event.timestamp);
    }
}

// Create and start the test
var voiceTest = new VoiceTestAgent({ name: "VoiceTest" });
emit test.start;

print("ğŸ¯ Voice input/output pipeline test initiated");
print("ğŸ¤ Testing full voice processing: Input â†’ Processing â†’ Output");
print("ğŸ”Š Using direct hardware control with Dr. Thorne's optimizations");
print("Press [Enter] to exit...");
