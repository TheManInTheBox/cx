// Voice Output Service Test - Priority Queuing & Direct Hardware Control
// Focus: Test voice synthesis and hardware audio playback with debugging

conscious VoiceTestAgent
{
    realize(self: conscious)
    {
        learn self;
        emit agent.ready { name: self.name };
    }
    
    on test.start (event)
    {
        print("🔊 Starting voice output service test");
        print("� Focus: Voice synthesis → Hardware playback");
        
        // Test 1: Direct voice synthesis
        emit realtime.connect { demo: "voice_output_test" };
    }
    
    on realtime.connected (event)
    {
        print("✅ Connected to Azure Realtime API");
        emit realtime.session.create { mode: "voice" };
    }
    
    on realtime.session.created (event)
    {
        print("✅ Voice session created");
        print("🗣️ Requesting voice synthesis...");
        
        // Request voice synthesis with clear debugging
        emit realtime.text.send { 
            text: "Testing voice output. You should hear this spoken audio."
        };
    }
    
    on realtime.voice.response (event)
    {
        is {
            context: "Should process voice data?",
            evaluate: "Voice data is available for playback",
            data: { audioData: event.audioData },
            handlers: [ voice.data.received ]
        };
        
        is {
            context: "Should log completion status?",
            evaluate: "Voice synthesis is complete",
            data: { isComplete: event.isComplete },
            handlers: [ voice.synthesis.completed ]
        };
    }
    
    on voice.data.received (event)
    {
        print("🔊 Received voice data - routing to VoiceOutputService");
        print("📊 Voice data type: " + typeof(event.audioData));
        
        // Route voice data to VoiceOutputService for playback
        emit voice.output.play { 
            audioData: event.audioData,
            sampleRate: 24000,
            channels: 1
        };
    }
    
    on voice.synthesis.completed (event)
    {
        print("🎵 Voice synthesis complete!");
    }
    
    on voice.output.queued (event)
    {
        print("🔄 Voice request queued:");
        print("  Request ID: " + event.requestId);
        print("  Priority: " + event.priority);
        print("  Audio Length: " + event.audioLength + " bytes");
    }
    
    on voice.output.started (event)
    {
        print("✅ Voice output started successfully:");
        print("  Request ID: " + event.requestId);
        print("  Priority: " + event.priority);
        print("  Audio length: " + event.audioLength + " bytes");
        print("  Sample rate: " + event.sampleRate + "Hz");
        print("  Channels: " + event.channels);
    }
    
    on voice.output.completed (event)
    {
        print("✅ Voice output completed successfully:");
        print("  Request ID: " + event.requestId);
        print("  Priority: " + event.priority);
        print("  Timestamp: " + event.timestamp);
        
        is {
            context: "Should display exception information?",
            evaluate: "Event contains exception data",
            data: { exception: event.exception },
            handlers: [ exception.display ]
        };
    }
    
    on exception.display (event)
    {
        print("⚠️ Exception during playback: " + event.exception);
    }
    
    on voice.output.error (event)
    {
        print("❌ Voice output error: " + event.error);
        print("Timestamp: " + event.timestamp);
    }
}

// Create and start the test
var voiceTest = new VoiceTestAgent({ name: "VoiceTest" });
emit test.start;

print("🎯 Voice input/output pipeline test initiated");
print("🎤 Testing full voice processing: Input → Processing → Output");
print("🔊 Using direct hardware control with Dr. Thorne's optimizations");
print("Press [Enter] to exit...");
